{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 74,
      "id": "662a93d1",
      "metadata": {
        "id": "662a93d1",
        "outputId": "00355c05-d836-4358-9665-898077694d06",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-science-training-series'...\n",
            "remote: Enumerating objects: 1809, done.\u001b[K\n",
            "remote: Counting objects: 100% (427/427), done.\u001b[K\n",
            "remote: Compressing objects: 100% (157/157), done.\u001b[K\n",
            "remote: Total 1809 (delta 308), reused 359 (delta 268), pack-reused 1382\u001b[K\n",
            "Receiving objects: 100% (1809/1809), 202.41 MiB | 32.23 MiB/s, done.\n",
            "Resolving deltas: 100% (891/891), done.\n",
            "Checking out files: 100% (240/240), done.\n",
            "/content/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python/ai-science-training-series/02_neural_networks_python\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/argonne-lcf/ai-science-training-series.git\n",
        "%cd ai-science-training-series/02_neural_networks_python/\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "id": "e19878bb",
      "metadata": {
        "id": "e19878bb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "id": "da412dba",
      "metadata": {
        "id": "da412dba",
        "outputId": "b0fb1ce3-5a9f-47ed-e58d-bfaaea92da4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(60000, 28, 28)\n",
            "(60000, 784)\n",
            "\n",
            "MNIST data loaded: train: 60000 test: 10000\n",
            "X_train: (60000, 784)\n",
            "y_train: (60000,)\n"
          ]
        }
      ],
      "source": [
        "# repeating the data prep from the previous notebook\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(numpy.float32)\n",
        "x_test  = x_test.astype(numpy.float32)\n",
        "\n",
        "x_train /= 255.\n",
        "x_test  /= 255.\n",
        "\n",
        "print(x_train.shape)\n",
        "x_train = x_train.reshape(x_train.shape[0], numpy.prod(x_train[0,:,:].shape))\n",
        "x_test = x_test.reshape(x_test.shape[0], numpy.prod(x_test[0,:,:].shape))\n",
        "\n",
        "print(x_train.shape)\n",
        "y_train = y_train.astype(numpy.int32)\n",
        "y_test  = y_test.astype(numpy.int32)\n",
        "\n",
        "print()\n",
        "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
        "print('X_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "\n",
        "# one-hot encoding:\n",
        "nb_classes = 10\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "id": "302994b1",
      "metadata": {
        "id": "302994b1"
      },
      "outputs": [],
      "source": [
        "# Here we import an implementation of a two-layer neural network \n",
        "# this code is based on pieces of the first assignment from Stanford's CSE231n course, \n",
        "# hosted at https://github.com/cs231n/cs231n.github.io with the MIT license\n",
        "from fc_net import TwoLayerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "id": "4e00e3de",
      "metadata": {
        "id": "4e00e3de"
      },
      "outputs": [],
      "source": [
        "num_features = x_train.shape[1] # this is the number of pixels\n",
        "# The weights are initialized from a normal distribution with standard deviation weight_scale\n",
        "model = TwoLayerNet(input_dim=num_features, hidden_dim=100, num_classes=nb_classes, weight_scale=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "id": "32f7f1aa",
      "metadata": {
        "id": "32f7f1aa"
      },
      "outputs": [],
      "source": [
        "# here you can take a look if you want at the initial loss from an untrained network\n",
        "loss, gradients = model.loss(x_train, y_train_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "id": "c43e3aa5",
      "metadata": {
        "id": "c43e3aa5"
      },
      "outputs": [],
      "source": [
        "# a simple implementation of stochastic gradient descent\n",
        "def sgd(model, gradients, learning_rate):\n",
        "    for p, w in model.params.items():\n",
        "        dw = gradients[p]\n",
        "        new_weights = w - learning_rate * dw\n",
        "        model.params[p] = new_weights\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "id": "c8316228",
      "metadata": {
        "id": "c8316228"
      },
      "outputs": [],
      "source": [
        "# one training step\n",
        "def learn(model, x_train, y_train_onehot, learning_rate):\n",
        "    loss, gradients = model.loss(x_train, y_train_onehot)\n",
        "    model = sgd(model, gradients, learning_rate)\n",
        "    return loss, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "id": "81886e8c",
      "metadata": {
        "id": "81886e8c"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, x, true_values):\n",
        "    scores = model.loss(x)\n",
        "    predictions = numpy.argmax(scores, axis=1)\n",
        "    N = predictions.shape[0]\n",
        "    acc = (true_values == predictions).sum() / N\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "id": "49754891",
      "metadata": {
        "id": "49754891",
        "outputId": "807e9536-a6b5-47a8-f883-ae62d41cd4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0, loss 2.10854, accuracy 0.65\n",
            "epoch 1, loss 2.14016, accuracy 0.30\n",
            "Loss increased\n",
            "epoch 2, loss 1.01530, accuracy 0.68\n",
            "epoch 3, loss 0.73211, accuracy 0.74\n",
            "epoch 4, loss 0.69528, accuracy 0.77\n",
            "epoch 5, loss 0.47001, accuracy 0.87\n",
            "epoch 6, loss 0.43126, accuracy 0.88\n",
            "epoch 7, loss 0.39655, accuracy 0.88\n",
            "epoch 8, loss 0.38765, accuracy 0.89\n",
            "epoch 9, loss 0.37748, accuracy 0.89\n",
            "epoch 10, loss 0.37461, accuracy 0.89\n",
            "epoch 11, loss 0.37552, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 12, loss 0.38015, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 13, loss 0.36694, accuracy 0.89\n",
            "epoch 14, loss 0.37266, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 15, loss 0.37087, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 16, loss 0.37083, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 17, loss 0.36553, accuracy 0.89\n",
            "epoch 18, loss 0.37721, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 19, loss 0.36657, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 20, loss 0.37412, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 21, loss 0.36538, accuracy 0.89\n",
            "epoch 22, loss 0.37437, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 23, loss 0.36856, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 24, loss 0.37355, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 25, loss 0.35756, accuracy 0.89\n",
            "epoch 26, loss 0.36579, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 27, loss 0.37834, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 28, loss 0.37600, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 29, loss 0.38052, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 30, loss 0.36594, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 31, loss 0.36447, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 32, loss 0.36679, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 33, loss 0.37210, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 34, loss 0.36976, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 35, loss 0.37062, accuracy 0.89\n",
            "Loss increased\n",
            "epoch 36, loss 0.37040, accuracy 0.89\n",
            "Loss increased\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f349aa596d0>]"
            ]
          },
          "metadata": {},
          "execution_count": 83
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Here's an example training loop using this two-layer model. Can you do better? \n",
        "learning_rate = 1.0 #aggressive rate\n",
        "num_examples = x_train.shape[0]\n",
        "batch_size = 10000\n",
        "num_batches = int(num_examples / batch_size)\n",
        "#run a bunch of epochs\n",
        "num_epochs = 1000\n",
        "losses = numpy.zeros(num_batches*num_epochs,)\n",
        "indices = numpy.arange(num_examples)\n",
        "#but break if nothing is happening\n",
        "noincrease_cnt = 0\n",
        "noincrease_max = 10\n",
        "last_loss = 100.0 #will hold lowest loss\n",
        "i = 0\n",
        "for epoch in range(0, num_epochs):\n",
        "    # in each epoch, we loop over all of the training examples\n",
        "    learning_rate=learning_rate*numpy.exp(-8*numpy.log(2)*i/num_epochs) #quickly lower loss\n",
        "    for step in range(0, num_batches):\n",
        "        # grabbing the next batch\n",
        "        offset = step * batch_size\n",
        "        batch_range = range(offset, offset+batch_size)\n",
        "        x_train_batch = x_train[batch_range, :]\n",
        "        y_train_batch = y_train_onehot[batch_range,:]\n",
        "        \n",
        "        # feed the next batch in to do one sgd step\n",
        "        loss, model = learn(model, x_train_batch, y_train_batch, learning_rate)\n",
        "        losses[i] = loss\n",
        "        i += 1\n",
        "\n",
        "    acc = accuracy(model, x_train, y_train)\n",
        "    print(\"epoch %d, loss %.5f, accuracy %.2f\" % (epoch, loss, acc))\n",
        "    \n",
        "    # reshuffle the data so that we get a new set of batches\n",
        "    numpy.random.shuffle(indices)\n",
        "    x_train = x_train[indices,:]\n",
        "    y_train = y_train[indices] # keep this shuffled the same way for use in accuracy calculation\n",
        "    y_train_onehot = y_train_onehot[indices,:]\n",
        "    #check if losses aren't improving and then just break\n",
        "    if i>1 and loss > last_loss: \n",
        "      print(\"Loss increased\")\n",
        "      noincrease_cnt = noincrease_cnt + 1\n",
        "    else:\n",
        "      last_loss = loss\n",
        "      noincrease_cnt = 0\n",
        "    if noincrease_cnt > noincrease_max: break\n",
        "plt.plot(losses)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "a4f274c6",
      "metadata": {
        "id": "a4f274c6",
        "outputId": "1be4e29f-17d7-4e96-fe5c-6e0b91f1c717",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8938333333333334"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "accuracy(model, x_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dd5728",
      "metadata": {
        "id": "a2dd5728"
      },
      "source": [
        "# Homework: improve the accuracy of this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faaf0515",
      "metadata": {
        "id": "faaf0515"
      },
      "source": [
        "Update this notebook so that the accuracy is improved. How high can you get it? You could change things directly in the notebook, such as increasing the number of epochs, changing the learning weight, changing the width of the hidden layer, etc. If you're more ambitious, you could also try changing the model definition itself by checking out the associated Python files. For example, you could add more layers to the network. The current notebook has a training accuracy of about 43%, but will vary with randomness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "id": "6e484c13",
      "metadata": {
        "id": "6e484c13"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}