{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "662a93d1",
      "metadata": {
        "id": "662a93d1",
        "outputId": "0ec44892-4edf-486f-c606-27e1744b2bbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ai-science-training-series'...\n",
            "remote: Enumerating objects: 1809, done.\u001b[K\n",
            "remote: Counting objects: 100% (427/427), done.\u001b[K\n",
            "remote: Compressing objects: 100% (158/158), done.\u001b[K\n",
            "remote: Total 1809 (delta 308), reused 357 (delta 267), pack-reused 1382\u001b[K\n",
            "Receiving objects: 100% (1809/1809), 202.41 MiB | 24.73 MiB/s, done.\n",
            "Resolving deltas: 100% (891/891), done.\n",
            "Checking out files: 100% (240/240), done.\n",
            "/content/ai-science-training-series/02_neural_networks_python\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/argonne-lcf/ai-science-training-series.git\n",
        "%cd ai-science-training-series/02_neural_networks_python/\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "e19878bb",
      "metadata": {
        "id": "e19878bb"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import numpy\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "da412dba",
      "metadata": {
        "id": "da412dba",
        "outputId": "9347d93d-2d9c-44e1-843a-5f303b138bb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "(60000, 28, 28)\n",
            "(60000, 784)\n",
            "\n",
            "MNIST data loaded: train: 60000 test: 10000\n",
            "X_train: (60000, 784)\n",
            "y_train: (60000,)\n"
          ]
        }
      ],
      "source": [
        "# repeating the data prep from the previous notebook\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.astype(numpy.float32)\n",
        "x_test  = x_test.astype(numpy.float32)\n",
        "\n",
        "x_train /= 255.\n",
        "x_test  /= 255.\n",
        "\n",
        "print(x_train.shape)\n",
        "x_train = x_train.reshape(x_train.shape[0], numpy.prod(x_train[0,:,:].shape))\n",
        "x_test = x_test.reshape(x_test.shape[0], numpy.prod(x_test[0,:,:].shape))\n",
        "\n",
        "print(x_train.shape)\n",
        "y_train = y_train.astype(numpy.int32)\n",
        "y_test  = y_test.astype(numpy.int32)\n",
        "\n",
        "print()\n",
        "print('MNIST data loaded: train:',len(x_train),'test:',len(x_test))\n",
        "print('X_train:', x_train.shape)\n",
        "print('y_train:', y_train.shape)\n",
        "\n",
        "# one-hot encoding:\n",
        "nb_classes = 10\n",
        "y_train_onehot = tf.keras.utils.to_categorical(y_train, nb_classes)\n",
        "y_test_onehot = tf.keras.utils.to_categorical(y_test, nb_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "302994b1",
      "metadata": {
        "id": "302994b1"
      },
      "outputs": [],
      "source": [
        "# Here we import an implementation of a two-layer neural network \n",
        "# this code is based on pieces of the first assignment from Stanford's CSE231n course, \n",
        "# hosted at https://github.com/cs231n/cs231n.github.io with the MIT license\n",
        "from fc_net import TwoLayerNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "4e00e3de",
      "metadata": {
        "id": "4e00e3de"
      },
      "outputs": [],
      "source": [
        "num_features = x_train.shape[1] # this is the number of pixels\n",
        "# The weights are initialized from a normal distribution with standard deviation weight_scale\n",
        "model = TwoLayerNet(input_dim=num_features, hidden_dim=100, num_classes=nb_classes, weight_scale=.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "32f7f1aa",
      "metadata": {
        "id": "32f7f1aa"
      },
      "outputs": [],
      "source": [
        "# here you can take a look if you want at the initial loss from an untrained network\n",
        "loss, gradients = model.loss(x_train, y_train_onehot)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "c43e3aa5",
      "metadata": {
        "id": "c43e3aa5"
      },
      "outputs": [],
      "source": [
        "# a simple implementation of stochastic gradient descent\n",
        "def sgd(model, gradients, learning_rate):\n",
        "    for p, w in model.params.items():\n",
        "        dw = gradients[p]\n",
        "        new_weights = w - learning_rate * dw\n",
        "        model.params[p] = new_weights\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c8316228",
      "metadata": {
        "id": "c8316228"
      },
      "outputs": [],
      "source": [
        "# one training step\n",
        "def learn(model, x_train, y_train_onehot, learning_rate):\n",
        "    loss, gradients = model.loss(x_train, y_train_onehot)\n",
        "    model = sgd(model, gradients, learning_rate)\n",
        "    return loss, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "81886e8c",
      "metadata": {
        "id": "81886e8c"
      },
      "outputs": [],
      "source": [
        "def accuracy(model, x, true_values):\n",
        "    scores = model.loss(x)\n",
        "    predictions = numpy.argmax(scores, axis=1)\n",
        "    N = predictions.shape[0]\n",
        "    acc = (true_values == predictions).sum() / N\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49754891",
      "metadata": {
        "id": "49754891",
        "outputId": "34249f92-c822-4e55-a209-3f1ad934633c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dim 1, epoch 46, loss 1.92722, accuracy 0.23\n",
            "dim 2, epoch 24, loss 1.52544, accuracy 0.48\n",
            "dim 3, epoch 30, loss 1.16603, accuracy 0.61\n",
            "dim 4, epoch 42, loss 0.82920, accuracy 0.74\n",
            "dim 5, epoch 45, loss 0.98110, accuracy 0.67\n",
            "dim 6, epoch 40, loss 1.10110, accuracy 0.64\n",
            "dim 7, epoch 28, loss 0.65468, accuracy 0.81\n",
            "dim 8, epoch 35, loss 0.65885, accuracy 0.81\n",
            "dim 9, epoch 38, loss 0.80631, accuracy 0.77\n",
            "dim 10, epoch 35, loss 0.78834, accuracy 0.77\n",
            "dim 11, epoch 44, loss 0.51884, accuracy 0.85\n",
            "dim 12, epoch 22, loss 0.50747, accuracy 0.86\n",
            "dim 13, epoch 33, loss 0.73206, accuracy 0.80\n",
            "dim 14, epoch 23, loss 0.54663, accuracy 0.85\n",
            "dim 15, epoch 30, loss 0.51447, accuracy 0.85\n",
            "dim 16, epoch 30, loss 0.49415, accuracy 0.86\n",
            "dim 17, epoch 24, loss 0.55573, accuracy 0.85\n",
            "dim 18, epoch 22, loss 0.48997, accuracy 0.87\n",
            "dim 19, epoch 23, loss 0.59558, accuracy 0.83\n",
            "dim 20, epoch 23, loss 0.47922, accuracy 0.87\n",
            "dim 21, epoch 36, loss 0.51688, accuracy 0.86\n",
            "dim 22, epoch 23, loss 0.49295, accuracy 0.87\n",
            "dim 23, epoch 24, loss 0.51696, accuracy 0.86\n",
            "dim 24, epoch 35, loss 0.46966, accuracy 0.87\n",
            "dim 25, epoch 32, loss 0.55680, accuracy 0.84\n",
            "dim 26, epoch 30, loss 0.49897, accuracy 0.86\n",
            "dim 27, epoch 38, loss 0.47263, accuracy 0.87\n",
            "dim 28, epoch 31, loss 0.51694, accuracy 0.85\n",
            "dim 29, epoch 22, loss 0.52185, accuracy 0.85\n",
            "dim 30, epoch 32, loss 0.48834, accuracy 0.87\n",
            "dim 31, epoch 46, loss 0.49074, accuracy 0.86\n",
            "dim 32, epoch 23, loss 0.45490, accuracy 0.88\n",
            "dim 33, epoch 45, loss 0.45016, accuracy 0.87\n",
            "dim 34, epoch 35, loss 0.47339, accuracy 0.87\n",
            "dim 35, epoch 33, loss 0.50939, accuracy 0.86\n",
            "dim 36, epoch 31, loss 0.43189, accuracy 0.88\n",
            "dim 37, epoch 30, loss 0.47509, accuracy 0.87\n",
            "dim 38, epoch 26, loss 0.44580, accuracy 0.88\n",
            "dim 39, epoch 22, loss 0.45602, accuracy 0.88\n",
            "dim 40, epoch 32, loss 0.45320, accuracy 0.87\n",
            "dim 41, epoch 21, loss 0.44572, accuracy 0.88\n",
            "dim 42, epoch 26, loss 0.50688, accuracy 0.86\n",
            "dim 43, epoch 22, loss 0.45299, accuracy 0.88\n",
            "dim 44, epoch 21, loss 0.46475, accuracy 0.87\n",
            "dim 45, epoch 25, loss 0.42841, accuracy 0.88\n",
            "dim 46, epoch 36, loss 0.45246, accuracy 0.88\n",
            "dim 47, epoch 26, loss 0.46756, accuracy 0.87\n",
            "dim 48, epoch 52, loss 0.45051, accuracy 0.88\n",
            "dim 49, epoch 31, loss 0.47558, accuracy 0.87\n",
            "dim 50, epoch 33, loss 0.45716, accuracy 0.87\n",
            "dim 51, epoch 63, loss 0.47448, accuracy 0.87\n",
            "dim 52, epoch 29, loss 0.49666, accuracy 0.86\n",
            "dim 53, epoch 28, loss 0.44529, accuracy 0.88\n",
            "dim 54, epoch 22, loss 0.43596, accuracy 0.88\n",
            "dim 55, epoch 37, loss 0.45150, accuracy 0.87\n",
            "dim 56, epoch 35, loss 0.42342, accuracy 0.88\n",
            "dim 57, epoch 26, loss 0.41674, accuracy 0.88\n",
            "dim 58, epoch 31, loss 0.49097, accuracy 0.86\n",
            "dim 59, epoch 28, loss 0.43262, accuracy 0.88\n",
            "dim 60, epoch 45, loss 0.46579, accuracy 0.87\n",
            "dim 61, epoch 29, loss 0.44501, accuracy 0.88\n",
            "dim 62, epoch 32, loss 0.48746, accuracy 0.86\n",
            "dim 63, epoch 34, loss 0.44268, accuracy 0.87\n",
            "dim 64, epoch 28, loss 0.46955, accuracy 0.87\n",
            "dim 65, epoch 37, loss 0.45370, accuracy 0.87\n",
            "dim 66, epoch 26, loss 0.45645, accuracy 0.88\n",
            "dim 67, epoch 52, loss 0.46179, accuracy 0.87\n",
            "dim 68, epoch 49, loss 0.43687, accuracy 0.88\n",
            "dim 69, epoch 25, loss 0.44184, accuracy 0.88\n",
            "dim 70, epoch 21, loss 0.46120, accuracy 0.87\n",
            "dim 71, epoch 21, loss 0.45255, accuracy 0.88\n",
            "dim 72, epoch 21, loss 0.42936, accuracy 0.88\n",
            "dim 73, epoch 34, loss 0.42790, accuracy 0.88\n",
            "dim 74, epoch 34, loss 0.47300, accuracy 0.87\n",
            "dim 75, epoch 25, loss 0.42916, accuracy 0.88\n",
            "dim 76, epoch 45, loss 0.45082, accuracy 0.87\n",
            "dim 77, epoch 22, loss 0.45963, accuracy 0.87\n",
            "dim 78, epoch 34, loss 0.43273, accuracy 0.88\n",
            "dim 79, epoch 39, loss 0.43082, accuracy 0.88\n",
            "dim 80, epoch 40, loss 0.44199, accuracy 0.88\n",
            "dim 81, epoch 22, loss 0.44478, accuracy 0.88\n",
            "dim 82, epoch 36, loss 0.45009, accuracy 0.88\n",
            "dim 83, epoch 21, loss 0.46381, accuracy 0.87\n",
            "dim 84, epoch 26, loss 0.46305, accuracy 0.87\n",
            "dim 85, epoch 35, loss 0.42074, accuracy 0.88\n",
            "dim 86, epoch 22, loss 0.43851, accuracy 0.88\n",
            "dim 87, epoch 33, loss 0.42869, accuracy 0.88\n",
            "dim 88, epoch 30, loss 0.43281, accuracy 0.88\n",
            "dim 89, epoch 22, loss 0.40913, accuracy 0.89\n",
            "dim 90, epoch 29, loss 0.43200, accuracy 0.88\n",
            "dim 91, epoch 37, loss 0.44165, accuracy 0.88\n",
            "dim 92, epoch 37, loss 0.44103, accuracy 0.88\n",
            "dim 93, epoch 47, loss 0.44397, accuracy 0.88\n",
            "dim 94, epoch 33, loss 0.45585, accuracy 0.87\n",
            "dim 95, epoch 31, loss 0.41885, accuracy 0.88\n",
            "dim 96, epoch 22, loss 0.43104, accuracy 0.88\n",
            "dim 97, epoch 33, loss 0.41238, accuracy 0.88\n",
            "dim 98, epoch 52, loss 0.44368, accuracy 0.87\n",
            "dim 99, epoch 26, loss 0.44897, accuracy 0.87\n",
            "dim 100, epoch 42, loss 0.41747, accuracy 0.88\n",
            "dim 101, epoch 25, loss 0.42400, accuracy 0.88\n",
            "dim 102, epoch 44, loss 0.44829, accuracy 0.88\n",
            "dim 103, epoch 24, loss 0.44135, accuracy 0.88\n",
            "dim 104, epoch 23, loss 0.47307, accuracy 0.87\n",
            "dim 105, epoch 38, loss 0.43705, accuracy 0.88\n",
            "dim 106, epoch 28, loss 0.44661, accuracy 0.88\n",
            "dim 107, epoch 37, loss 0.41798, accuracy 0.88\n",
            "dim 108, epoch 25, loss 0.45318, accuracy 0.88\n",
            "dim 109, epoch 29, loss 0.44402, accuracy 0.88\n",
            "dim 110, epoch 31, loss 0.43742, accuracy 0.88\n",
            "dim 111, epoch 24, loss 0.46177, accuracy 0.87\n",
            "dim 112, epoch 40, loss 0.42327, accuracy 0.88\n",
            "dim 113, epoch 35, loss 0.43105, accuracy 0.88\n",
            "dim 114, epoch 28, loss 0.45026, accuracy 0.88\n",
            "dim 115, epoch 30, loss 0.42320, accuracy 0.88\n",
            "dim 116, epoch 56, loss 0.44381, accuracy 0.88\n",
            "dim 117, epoch 21, loss 0.41719, accuracy 0.88\n",
            "dim 118, epoch 23, loss 0.45346, accuracy 0.88\n",
            "dim 119, epoch 28, loss 0.43750, accuracy 0.88\n",
            "dim 120, epoch 27, loss 0.42020, accuracy 0.88\n",
            "dim 121, epoch 26, loss 0.41270, accuracy 0.88\n",
            "dim 122, epoch 24, loss 0.42467, accuracy 0.88\n",
            "dim 123, epoch 30, loss 0.43252, accuracy 0.88\n",
            "dim 124, epoch 22, loss 0.42310, accuracy 0.88\n",
            "dim 125, epoch 28, loss 0.44040, accuracy 0.88\n",
            "dim 126, epoch 42, loss 0.43231, accuracy 0.88\n",
            "dim 127, epoch 21, loss 0.42572, accuracy 0.88\n",
            "dim 128, epoch 42, loss 0.43724, accuracy 0.88\n",
            "dim 129, epoch 24, loss 0.41809, accuracy 0.88\n",
            "dim 130, epoch 31, loss 0.41004, accuracy 0.88\n",
            "dim 131, epoch 31, loss 0.41683, accuracy 0.89\n",
            "dim 132, epoch 35, loss 0.42628, accuracy 0.88\n",
            "dim 133, epoch 24, loss 0.41950, accuracy 0.88\n",
            "dim 134, epoch 39, loss 0.46583, accuracy 0.87\n",
            "dim 135, epoch 29, loss 0.43438, accuracy 0.88\n",
            "dim 136, epoch 37, loss 0.41899, accuracy 0.88\n",
            "dim 137, epoch 33, loss 0.42644, accuracy 0.88\n",
            "dim 138, epoch 22, loss 0.42580, accuracy 0.88\n",
            "dim 139, epoch 32, loss 0.42390, accuracy 0.88\n",
            "dim 140, epoch 35, loss 0.43997, accuracy 0.88\n",
            "dim 141, epoch 26, loss 0.40034, accuracy 0.89\n",
            "dim 142, epoch 21, loss 0.41452, accuracy 0.88\n",
            "dim 143, epoch 31, loss 0.40786, accuracy 0.89\n",
            "dim 144, epoch 21, loss 0.41575, accuracy 0.89\n",
            "dim 145, epoch 25, loss 0.43090, accuracy 0.88\n",
            "dim 146, epoch 34, loss 0.42294, accuracy 0.88\n",
            "dim 147, epoch 21, loss 0.41166, accuracy 0.88\n",
            "dim 148, epoch 23, loss 0.42555, accuracy 0.88\n",
            "dim 149, epoch 20, loss 0.42693, accuracy 0.88\n",
            "dim 150, epoch 25, loss 0.43107, accuracy 0.88\n",
            "dim 151, epoch 32, loss 0.42094, accuracy 0.88\n",
            "dim 152, epoch 23, loss 0.43117, accuracy 0.88\n",
            "dim 153, epoch 21, loss 0.41608, accuracy 0.88\n",
            "dim 154, epoch 38, loss 0.41106, accuracy 0.89\n",
            "dim 155, epoch 41, loss 0.42439, accuracy 0.88\n",
            "dim 156, epoch 36, loss 0.41883, accuracy 0.88\n",
            "dim 157, epoch 26, loss 0.39924, accuracy 0.89\n",
            "dim 158, epoch 30, loss 0.40879, accuracy 0.89\n",
            "dim 159, epoch 35, loss 0.42014, accuracy 0.88\n",
            "dim 160, epoch 25, loss 0.40560, accuracy 0.88\n",
            "dim 161, epoch 27, loss 0.41359, accuracy 0.88\n",
            "dim 162, epoch 25, loss 0.42108, accuracy 0.88\n",
            "dim 163, epoch 22, loss 0.41073, accuracy 0.89\n",
            "dim 164, epoch 27, loss 0.42861, accuracy 0.88\n",
            "dim 165, epoch 22, loss 0.41105, accuracy 0.88\n",
            "dim 166, epoch 40, loss 0.41898, accuracy 0.88\n",
            "dim 167, epoch 33, loss 0.41102, accuracy 0.88\n",
            "dim 168, epoch 22, loss 0.42861, accuracy 0.88\n",
            "dim 169, epoch 51, loss 0.42645, accuracy 0.88\n",
            "dim 170, epoch 31, loss 0.44103, accuracy 0.88\n",
            "dim 171, epoch 26, loss 0.42058, accuracy 0.88\n",
            "dim 172, epoch 26, loss 0.41550, accuracy 0.88\n",
            "dim 173, epoch 35, loss 0.41119, accuracy 0.88\n",
            "dim 174, epoch 20, loss 0.41016, accuracy 0.89\n",
            "dim 175, epoch 30, loss 0.40601, accuracy 0.89\n",
            "dim 176, epoch 31, loss 0.42091, accuracy 0.88\n",
            "dim 177, epoch 25, loss 0.42430, accuracy 0.89\n",
            "dim 178, epoch 33, loss 0.40576, accuracy 0.89\n",
            "dim 179, epoch 28, loss 0.42559, accuracy 0.88\n",
            "dim 180, epoch 34, loss 0.42334, accuracy 0.88\n",
            "dim 181, epoch 26, loss 0.41090, accuracy 0.88\n",
            "dim 182, epoch 29, loss 0.40488, accuracy 0.89\n",
            "dim 183, epoch 21, loss 0.39428, accuracy 0.89\n",
            "dim 184, epoch 23, loss 0.42607, accuracy 0.88\n",
            "dim 185, epoch 38, loss 0.41478, accuracy 0.88\n",
            "dim 186, epoch 32, loss 0.41124, accuracy 0.88\n",
            "dim 187, epoch 40, loss 0.41628, accuracy 0.88\n",
            "dim 188, epoch 26, loss 0.40619, accuracy 0.89\n",
            "dim 189, epoch 30, loss 0.40854, accuracy 0.89\n",
            "dim 190, epoch 31, loss 0.41922, accuracy 0.89\n",
            "dim 191, epoch 24, loss 0.40845, accuracy 0.89\n",
            "dim 192, epoch 32, loss 0.39849, accuracy 0.89\n",
            "dim 193, epoch 25, loss 0.41064, accuracy 0.89\n",
            "dim 194, epoch 29, loss 0.44096, accuracy 0.88\n",
            "dim 195, epoch 31, loss 0.40601, accuracy 0.89\n",
            "dim 196, epoch 26, loss 0.40778, accuracy 0.89\n",
            "dim 197, epoch 36, loss 0.40930, accuracy 0.89\n",
            "dim 198, epoch 37, loss 0.43289, accuracy 0.88\n",
            "dim 199, epoch 27, loss 0.40430, accuracy 0.89\n",
            "dim 200, epoch 42, loss 0.40269, accuracy 0.88\n",
            "dim 201, epoch 21, loss 0.40542, accuracy 0.89\n",
            "dim 202, epoch 30, loss 0.39831, accuracy 0.89\n",
            "dim 203, epoch 22, loss 0.41218, accuracy 0.88\n",
            "dim 204, epoch 30, loss 0.40050, accuracy 0.89\n",
            "dim 205, epoch 31, loss 0.42150, accuracy 0.88\n",
            "dim 206, epoch 45, loss 0.41240, accuracy 0.89\n",
            "dim 207, epoch 31, loss 0.41503, accuracy 0.88\n",
            "dim 208, epoch 22, loss 0.40954, accuracy 0.89\n",
            "dim 209, epoch 22, loss 0.41505, accuracy 0.88\n",
            "dim 210, epoch 36, loss 0.40857, accuracy 0.88\n",
            "dim 211, epoch 31, loss 0.43254, accuracy 0.88\n",
            "dim 212, epoch 34, loss 0.38677, accuracy 0.89\n",
            "dim 213, epoch 32, loss 0.41449, accuracy 0.88\n",
            "dim 214, epoch 24, loss 0.40999, accuracy 0.88\n",
            "dim 215, epoch 50, loss 0.41343, accuracy 0.88\n",
            "dim 216, epoch 27, loss 0.41794, accuracy 0.88\n",
            "dim 217, epoch 27, loss 0.39437, accuracy 0.89\n",
            "dim 218, epoch 39, loss 0.40278, accuracy 0.89\n",
            "dim 219, epoch 25, loss 0.40854, accuracy 0.89\n",
            "dim 220, epoch 31, loss 0.40157, accuracy 0.89\n",
            "dim 221, epoch 31, loss 0.40684, accuracy 0.89\n",
            "dim 222, epoch 39, loss 0.41027, accuracy 0.88\n",
            "dim 223, epoch 32, loss 0.41518, accuracy 0.89\n",
            "dim 224, epoch 21, loss 0.40400, accuracy 0.89\n",
            "dim 225, epoch 54, loss 0.41457, accuracy 0.88\n",
            "dim 226, epoch 21, loss 0.40835, accuracy 0.89\n",
            "dim 227, epoch 24, loss 0.40990, accuracy 0.88\n",
            "dim 228, epoch 37, loss 0.40432, accuracy 0.89\n",
            "dim 229, epoch 24, loss 0.39735, accuracy 0.89\n",
            "dim 230, epoch 22, loss 0.39925, accuracy 0.89\n",
            "dim 231, epoch 35, loss 0.39799, accuracy 0.89\n",
            "dim 232, epoch 32, loss 0.41378, accuracy 0.88\n",
            "dim 233, epoch 24, loss 0.39670, accuracy 0.89\n",
            "dim 234, epoch 39, loss 0.40554, accuracy 0.89\n",
            "dim 235, epoch 21, loss 0.40627, accuracy 0.89\n",
            "dim 236, epoch 26, loss 0.40465, accuracy 0.89\n",
            "dim 237, epoch 25, loss 0.41947, accuracy 0.89\n",
            "dim 238, epoch 42, loss 0.43660, accuracy 0.88\n",
            "dim 239, epoch 24, loss 0.40875, accuracy 0.89\n",
            "dim 240, epoch 27, loss 0.41034, accuracy 0.89\n",
            "dim 241, epoch 23, loss 0.41110, accuracy 0.89\n",
            "dim 242, epoch 36, loss 0.40360, accuracy 0.89\n"
          ]
        }
      ],
      "source": [
        "# Here's an example training loop using this two-layer model. Can you do better? \n",
        "# Let's just try a parameter scan over hidden dimensions, and run epochs until it seems to stall\n",
        "# Maybe up to 500 is good? (And maybe can be computed in reasonable time?)\n",
        "for this_hidden_dim in range(1,501):\n",
        "  model = TwoLayerNet(input_dim=num_features, hidden_dim=this_hidden_dim, num_classes=nb_classes, weight_scale=.01)\n",
        "  learning_rate = 1.0 #start with an aggressive rate and we'll drop it\n",
        "  num_examples = x_train.shape[0]\n",
        "  batch_size = 10000\n",
        "  num_batches = int(num_examples / batch_size)\n",
        "  #run a bunch of epochs, but probably will stop before it gets to this (hopefully)\n",
        "  num_epochs = 100\n",
        "  losses = numpy.zeros(num_batches*num_epochs,)\n",
        "  indices = numpy.arange(num_examples)\n",
        "  #variables to check if we've stalled\n",
        "  noincrease_cnt = 0\n",
        "  noincrease_max = 10 #arbitrary max number of trials to accept in a row with no decrease in max loss\n",
        "  last_loss = 100.0 #will hold lowest loss\n",
        "  for i,epoch in enumerate(range(0, num_epochs)):\n",
        "      # in each epoch, we loop over all of the training examples\n",
        "      learning_rate=learning_rate*numpy.exp(-8*numpy.log(2)*i/num_epochs) #exponentially decay learning rate\n",
        "      for j,step in enumerate(range(0, num_batches)):\n",
        "          # grabbing the next batch\n",
        "          offset = step * batch_size\n",
        "          batch_range = range(offset, offset+batch_size)\n",
        "          x_train_batch = x_train[batch_range, :]\n",
        "          y_train_batch = y_train_onehot[batch_range,:]\n",
        "          \n",
        "          # feed the next batch in to do one sgd step\n",
        "          loss, model = learn(model, x_train_batch, y_train_batch, learning_rate)\n",
        "          losses[j] = loss\n",
        "\n",
        "      acc = accuracy(model, x_train, y_train)\n",
        "      # reshuffle the data so that we get a new set of batches\n",
        "      numpy.random.shuffle(indices)\n",
        "      x_train = x_train[indices,:]\n",
        "      y_train = y_train[indices] # keep this shuffled the same way for use in accuracy calculation\n",
        "      y_train_onehot = y_train_onehot[indices,:]\n",
        "      #check if this loss is better than the best lost\n",
        "      if i>1 and loss > last_loss: \n",
        "        noincrease_cnt = noincrease_cnt + 1\n",
        "      else:\n",
        "        last_loss = loss #store best loss\n",
        "        noincrease_cnt = 0 #reset counter\n",
        "      #if we havent improved on our best loss in an arbitrary number of trials, give up  \n",
        "      if noincrease_cnt > noincrease_max: \n",
        "        print(\"dim %d, epoch %d, loss %.5f, accuracy %.2f\" % (this_hidden_dim, epoch, loss, acc))\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2dd5728",
      "metadata": {
        "id": "a2dd5728"
      },
      "source": [
        "# Homework: improve the accuracy of this model. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faaf0515",
      "metadata": {
        "id": "faaf0515"
      },
      "source": [
        "Update this notebook so that the accuracy is improved. How high can you get it? You could change things directly in the notebook, such as increasing the number of epochs, changing the learning weight, changing the width of the hidden layer, etc. If you're more ambitious, you could also try changing the model definition itself by checking out the associated Python files. For example, you could add more layers to the network. The current notebook has a training accuracy of about 43%, but will vary with randomness."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As can be seen from the above parameter search (which ended prematurely as Colab disconnected at about ~6 hours) accuracy by this method (chosen learning rate and decay, number of hidden layers, etc.) gets up to about 89 % for sufficiently many hidden dimensions, and around ~20 dimensions seems to be enough to achieve this."
      ],
      "metadata": {
        "id": "JhpSi2vcKlET"
      },
      "id": "JhpSi2vcKlET"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}